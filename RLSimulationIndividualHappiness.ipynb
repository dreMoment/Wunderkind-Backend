{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import gymnasium\n",
    "import gymnasium.spaces\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "import csv\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "register(\n",
    "    id='AppEnvironment-v0',\n",
    "    entry_point='RLSimulation:AppEnvironment',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_happiness_score(readingtime, activitytime, screentime, default = True):\n",
    "        model_default = pickle.load(open('modelh_default.pkl', 'rb')) if default else pickle.load(open('modelh_indiv.pkl', 'rb'))\n",
    "        df = pd.DataFrame({'screentime':[screentime], 'activitytime':[activitytime], 'readingtime':[readingtime]})\n",
    "        res = model_default.predict(df).astype(int)[0]\n",
    "        return max(1, min(100,res)) # Make a prediction\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indiv_dataset(ideal_reading_time, ideal_physical_activity):\n",
    "\n",
    "    if ideal_reading_time == 0:\n",
    "        ideal_reading_time = 0.1\n",
    "    if ideal_physical_activity == 0:\n",
    "        ideal_physical_activity = 0.1\n",
    "\n",
    "    ideal_screen_time = 2.0\n",
    "    screenW = 0.4\n",
    "    data = []\n",
    "\n",
    "    for i in range(100000):\n",
    "        \n",
    "        day = random.randint(1, 28)\n",
    "        month = random.randint(1, 12)\n",
    "        year = 2023\n",
    "        date_str = f\"{year}-{month:02d}-{day:02d}\"\n",
    "\n",
    "        #Generate random inputs in hours\n",
    "        screen_time = round(random.uniform(0, 5), 1)\n",
    "        physical_activity = round(random.uniform(0, 5), 1)\n",
    "        reading_time = round(random.uniform(0, 5), 1)\n",
    "\n",
    "        # Normalize the time inputs between 0 and 1 \n",
    "        normalized_screen_time = min(1, max(0, 1 - (screen_time / ideal_screen_time)))\n",
    "        normalized_physical_activity_default = min(1, physical_activity / ideal_physical_activity)\n",
    "        normalized_reading_time_default = min(1, reading_time / ideal_reading_time)\n",
    "        score_default = (screenW * normalized_screen_time) + (0.5 * normalized_physical_activity_default) + (0.5 * normalized_reading_time_default)\n",
    "        happiness_score_default = max(1, min(100, int(score_default*100) + round(random.uniform(-2, 2),0)))\n",
    "        data.append([date_str, screen_time, physical_activity, reading_time, happiness_score_default])\n",
    "    \n",
    "\n",
    "    with open('dummyh_indiv.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['date', 'screentime', 'activitytime', 'readingtime', 'hscore'])\n",
    "        writer.writerows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hmodel():\n",
    "    users = pd.read_csv('dummyh_indiv.csv')\n",
    "    y = users['hscore']\n",
    "    users.drop(columns='hscore', inplace=True)\n",
    "    x = users[['screentime', 'activitytime', 'readingtime']]\n",
    "\n",
    "    # Training the model\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "    model = MLPRegressor(hidden_layer_sizes=(10, 5), max_iter=1000, learning_rate_init=0.01, n_iter_no_change=100, tol=1e-4, verbose=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    print(model.score(x_test, y_test))\n",
    "\n",
    "    pickle.dump(model, open('modelh_indiv.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppEnvironment(gymnasium.Env):\n",
    "    assessment = True\n",
    "\n",
    "    def __init__(self):\n",
    "        self.observation_space = gymnasium.spaces.Box(low=0, high=100, shape=(5,), dtype=float)\n",
    "        #['readingtime', 'activitytime', 'screentime', 'self reported happiness']\n",
    "        self.observation = np.array([0.0, 0.0, 5.0, 0.0, float(generate_happiness_score(0, 0, 5))])\n",
    "        self.action_space = gymnasium.spaces.Discrete(10)\n",
    "        #['increase_reading', 'increase_activity', 'increase_screentime', 'decrease_reading', 'decrease_activity', 'decrease_screentime', 'self reported happiness 0', 'self reported happiness 1', 'self reported happinness 2', 'self reported happiness 3']\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        super().reset()\n",
    "        self.observation = np.array([0.0, 0.0, 5.0,0.0, float(generate_happiness_score(0, 0, 5))])\n",
    "        return self.observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update the state based on the action\n",
    "        new_obersevation = self.observation.copy()\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        #print(\"Action: \", action, \"Obersevation: \", new_obersevation)\n",
    "\n",
    "        if action == 0:\n",
    "            new_obersevation[0] += 0.5\n",
    "        elif action == 1:\n",
    "            new_obersevation[1] += 0.5\n",
    "        elif action == 2:\n",
    "            new_obersevation[2] += 0.5\n",
    "        elif action == 3:\n",
    "            new_obersevation[0] -= 0.5\n",
    "        elif action == 4:\n",
    "            new_obersevation[1] -= 0.5\n",
    "        elif action == 5:\n",
    "            new_obersevation[2] -= 0.5\n",
    "        elif action == 6:\n",
    "            new_obersevation[3] = 1\n",
    "        elif action == 7:\n",
    "            new_obersevation[3] = 2\n",
    "        elif action == 8:\n",
    "            new_obersevation[3] = 3\n",
    "        elif action == 9:\n",
    "            new_obersevation[3] = 4\n",
    "        \n",
    "        #print(\"New obersevation: \", new_obersevation)\n",
    "\n",
    "\n",
    "        if new_obersevation[0] < 0 or new_obersevation[1] < 0 or new_obersevation[2] < 0:\n",
    "            #print(\"Negative values\")\n",
    "            reward -= 1000\n",
    "            new_obersevation = [max(0,x) for x in new_obersevation]\n",
    "\n",
    "        \n",
    "        if sum(new_obersevation[0:3]) > 24:\n",
    "            #print(\"Sum of all activities is greater than 24\")\n",
    "            reward -= 1000\n",
    "\n",
    "        if sum(new_obersevation[0:3]) <= 24:\n",
    "            self.observation = new_obersevation\n",
    "\n",
    "        new_happiness = generate_happiness_score(self.observation[0], self.observation[1], self.observation[2], self.assessment)\n",
    "        new_happiness = 0.75 * new_happiness + 0.25 * 25 * self.observation[3]\n",
    "        reward = new_happiness - self.observation[4]\n",
    "        \n",
    "        #do not make big changes in happiness\n",
    "        if reward >= 20:\n",
    "            reward -= 10\n",
    "        \n",
    "        self.observation[4] = new_happiness\n",
    "\n",
    "        if new_happiness > 95:\n",
    "            done = True\n",
    "            #print(\"Happiness is greater than 95\")\n",
    "            reward += 100\n",
    "\n",
    "        #print(\"Action: \", action, \"Obersevation: \", self.observation, \"Reward: \", reward, \"Done: \", done)\n",
    "\n",
    "        info = {}\n",
    "        return self.observation, reward, done, info, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_environment = AppEnvironment()\n",
    "individual_environment = AppEnvironment()\n",
    "individual_environment.assessment = False\n",
    "\n",
    "\n",
    "def train_default_RL_model():\n",
    "    model = PPO('MlpPolicy', env=assessment_environment, learning_rate=0.0001)\n",
    "    model.learn(total_timesteps=10000, progress_bar=True)\n",
    "    return model\n",
    "\n",
    "def train_individual_RL_model():\n",
    "    model = PPO('MlpPolicy', env=individual_environment, learning_rate=0.0001)\n",
    "    model.learn(total_timesteps=10000, progress_bar=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished assesment period, reading time:  1.8 activity time:  0.3\n",
      "Dataset created\n",
      "Iteration 1, loss = 799.11771915\n",
      "Iteration 2, loss = 54.83652228\n",
      "Iteration 3, loss = 27.04992298\n",
      "Iteration 4, loss = 22.11363247\n",
      "Iteration 5, loss = 21.55189032\n",
      "Iteration 6, loss = 21.31949183\n",
      "Iteration 7, loss = 21.37473666\n",
      "Iteration 8, loss = 21.24494399\n",
      "Iteration 9, loss = 21.21080385\n",
      "Iteration 10, loss = 21.22829942\n",
      "Iteration 11, loss = 21.19357865\n",
      "Iteration 12, loss = 21.20664222\n",
      "Iteration 13, loss = 21.39576221\n",
      "Iteration 14, loss = 21.28839584\n",
      "Iteration 15, loss = 21.32514522\n",
      "Iteration 16, loss = 21.20789743\n",
      "Iteration 17, loss = 21.23986031\n",
      "Iteration 18, loss = 21.18973541\n",
      "Iteration 19, loss = 21.31772505\n",
      "Iteration 20, loss = 21.14020332\n",
      "Iteration 21, loss = 21.19310847\n",
      "Iteration 22, loss = 21.20775063\n",
      "Iteration 23, loss = 21.34380669\n",
      "Iteration 24, loss = 21.15615591\n",
      "Iteration 25, loss = 21.28549502\n",
      "Iteration 26, loss = 21.22792825\n",
      "Iteration 27, loss = 21.23493093\n",
      "Iteration 28, loss = 21.21481137\n",
      "Iteration 29, loss = 21.33978318\n",
      "Iteration 30, loss = 21.29231763\n",
      "Iteration 31, loss = 21.23755044\n",
      "Iteration 32, loss = 21.16864232\n",
      "Iteration 33, loss = 21.20624519\n",
      "Iteration 34, loss = 21.31198051\n",
      "Iteration 35, loss = 21.15602881\n",
      "Iteration 36, loss = 21.18987205\n",
      "Iteration 37, loss = 21.25798786\n",
      "Iteration 38, loss = 21.20475257\n",
      "Iteration 39, loss = 21.23585735\n",
      "Iteration 40, loss = 21.31509096\n",
      "Iteration 41, loss = 21.15453218\n",
      "Iteration 42, loss = 21.19339932\n",
      "Iteration 43, loss = 21.19532835\n",
      "Iteration 44, loss = 21.23776847\n",
      "Iteration 45, loss = 21.10276168\n",
      "Iteration 46, loss = 20.60567633\n",
      "Iteration 47, loss = 20.30868214\n",
      "Iteration 48, loss = 20.01313216\n",
      "Iteration 49, loss = 19.97727782\n",
      "Iteration 50, loss = 19.81404137\n",
      "Iteration 51, loss = 19.72583710\n",
      "Iteration 52, loss = 19.65436703\n",
      "Iteration 53, loss = 19.72524621\n",
      "Iteration 54, loss = 19.57070676\n",
      "Iteration 55, loss = 19.67332120\n",
      "Iteration 56, loss = 19.70945371\n",
      "Iteration 57, loss = 19.59490086\n",
      "Iteration 58, loss = 19.53578395\n",
      "Iteration 59, loss = 19.61168472\n",
      "Iteration 60, loss = 19.64744634\n",
      "Iteration 61, loss = 19.53631192\n",
      "Iteration 62, loss = 19.47585727\n",
      "Iteration 63, loss = 19.54543250\n",
      "Iteration 64, loss = 19.51001270\n",
      "Iteration 65, loss = 19.49271129\n",
      "Iteration 66, loss = 19.56460524\n",
      "Iteration 67, loss = 19.50381897\n",
      "Iteration 68, loss = 19.53621578\n",
      "Iteration 69, loss = 19.48996339\n",
      "Iteration 70, loss = 19.50712893\n",
      "Iteration 71, loss = 19.61054061\n",
      "Iteration 72, loss = 19.50941738\n",
      "Iteration 73, loss = 19.60762059\n",
      "Iteration 74, loss = 19.51492006\n",
      "Iteration 75, loss = 19.52116308\n",
      "Iteration 76, loss = 19.52451571\n",
      "Iteration 77, loss = 19.55035923\n",
      "Iteration 78, loss = 19.66739500\n",
      "Iteration 79, loss = 19.54201285\n",
      "Iteration 80, loss = 19.55450860\n",
      "Iteration 81, loss = 19.55618692\n",
      "Iteration 82, loss = 19.54070303\n",
      "Iteration 83, loss = 19.56707701\n",
      "Iteration 84, loss = 19.50351759\n",
      "Iteration 85, loss = 19.52279274\n",
      "Iteration 86, loss = 19.46453569\n",
      "Iteration 87, loss = 19.52766396\n",
      "Iteration 88, loss = 19.54588900\n",
      "Iteration 89, loss = 19.61946451\n",
      "Iteration 90, loss = 19.47044016\n",
      "Iteration 91, loss = 19.56036782\n",
      "Iteration 92, loss = 19.56378752\n",
      "Iteration 93, loss = 19.63670153\n",
      "Iteration 94, loss = 19.47566651\n",
      "Iteration 95, loss = 19.55309563\n",
      "Iteration 96, loss = 19.54610441\n",
      "Iteration 97, loss = 19.52405093\n",
      "Iteration 98, loss = 19.47629412\n",
      "Iteration 99, loss = 19.39957204\n",
      "Iteration 100, loss = 19.43810268\n",
      "Iteration 101, loss = 19.47225881\n",
      "Iteration 102, loss = 19.42093911\n",
      "Iteration 103, loss = 19.43494691\n",
      "Iteration 104, loss = 19.41998685\n",
      "Iteration 105, loss = 19.47649579\n",
      "Iteration 106, loss = 19.31620010\n",
      "Iteration 107, loss = 19.45262827\n",
      "Iteration 108, loss = 19.37681500\n",
      "Iteration 109, loss = 19.34424566\n",
      "Iteration 110, loss = 19.38930162\n",
      "Iteration 111, loss = 19.44970158\n",
      "Iteration 112, loss = 19.34220374\n",
      "Iteration 113, loss = 19.49497805\n",
      "Iteration 114, loss = 19.33814578\n",
      "Iteration 115, loss = 19.33341603\n",
      "Iteration 116, loss = 19.43919455\n",
      "Iteration 117, loss = 19.39163965\n",
      "Iteration 118, loss = 19.40666398\n",
      "Iteration 119, loss = 19.40275677\n",
      "Iteration 120, loss = 19.39824205\n",
      "Iteration 121, loss = 19.39474983\n",
      "Iteration 122, loss = 19.35615505\n",
      "Iteration 123, loss = 19.37623086\n",
      "Iteration 124, loss = 19.39011249\n",
      "Iteration 125, loss = 19.33572939\n",
      "Iteration 126, loss = 19.40623509\n",
      "Iteration 127, loss = 19.36000746\n",
      "Iteration 128, loss = 19.31931996\n",
      "Iteration 129, loss = 19.51480620\n",
      "Iteration 130, loss = 19.40347944\n",
      "Iteration 131, loss = 19.47610091\n",
      "Iteration 132, loss = 19.39845435\n",
      "Iteration 133, loss = 19.34801498\n",
      "Iteration 134, loss = 19.39664752\n",
      "Iteration 135, loss = 19.30197519\n",
      "Iteration 136, loss = 19.43322737\n",
      "Iteration 137, loss = 19.37547230\n",
      "Iteration 138, loss = 19.40422510\n",
      "Iteration 139, loss = 19.40226213\n",
      "Iteration 140, loss = 19.33886026\n",
      "Iteration 141, loss = 19.38945374\n",
      "Iteration 142, loss = 19.33922757\n",
      "Iteration 143, loss = 19.40819902\n",
      "Iteration 144, loss = 19.38599564\n",
      "Iteration 145, loss = 19.41226812\n",
      "Iteration 146, loss = 19.39536044\n",
      "Iteration 147, loss = 19.42190620\n",
      "Iteration 148, loss = 19.38677313\n",
      "Iteration 149, loss = 19.32122044\n",
      "Iteration 150, loss = 19.38706714\n",
      "Iteration 151, loss = 19.44317799\n",
      "Iteration 152, loss = 19.43884717\n",
      "Iteration 153, loss = 19.39492146\n",
      "Iteration 154, loss = 19.41245163\n",
      "Iteration 155, loss = 19.39684189\n",
      "Iteration 156, loss = 19.40290045\n",
      "Iteration 157, loss = 19.37601222\n",
      "Iteration 158, loss = 19.39770711\n",
      "Iteration 159, loss = 19.37980782\n",
      "Iteration 160, loss = 19.35517305\n",
      "Iteration 161, loss = 19.34474281\n",
      "Iteration 162, loss = 19.33402553\n",
      "Iteration 163, loss = 19.45044371\n",
      "Iteration 164, loss = 19.37278906\n",
      "Iteration 165, loss = 19.30810212\n",
      "Iteration 166, loss = 19.39079618\n",
      "Iteration 167, loss = 19.34929243\n",
      "Iteration 168, loss = 19.35544314\n",
      "Iteration 169, loss = 19.40254861\n",
      "Iteration 170, loss = 19.36361981\n",
      "Iteration 171, loss = 19.44191867\n",
      "Iteration 172, loss = 19.33444818\n",
      "Iteration 173, loss = 19.38871789\n",
      "Iteration 174, loss = 19.35159447\n",
      "Iteration 175, loss = 19.40997347\n",
      "Iteration 176, loss = 19.35088148\n",
      "Iteration 177, loss = 19.42652230\n",
      "Iteration 178, loss = 19.35360856\n",
      "Iteration 179, loss = 19.32928141\n",
      "Iteration 180, loss = 19.39183689\n",
      "Iteration 181, loss = 19.35483486\n",
      "Iteration 182, loss = 19.37444378\n",
      "Iteration 183, loss = 19.45645449\n",
      "Iteration 184, loss = 19.36726218\n",
      "Iteration 185, loss = 19.34246354\n",
      "Iteration 186, loss = 19.36240129\n",
      "Iteration 187, loss = 19.37904629\n",
      "Iteration 188, loss = 19.43505725\n",
      "Iteration 189, loss = 19.35462132\n",
      "Iteration 190, loss = 19.40421970\n",
      "Iteration 191, loss = 19.36367540\n",
      "Iteration 192, loss = 19.35719068\n",
      "Iteration 193, loss = 19.41264768\n",
      "Iteration 194, loss = 19.38252903\n",
      "Iteration 195, loss = 19.37602045\n",
      "Iteration 196, loss = 19.41018723\n",
      "Iteration 197, loss = 19.35899402\n",
      "Iteration 198, loss = 19.44044168\n",
      "Iteration 199, loss = 19.32275996\n",
      "Iteration 200, loss = 19.38718855\n",
      "Iteration 201, loss = 19.33884615\n",
      "Iteration 202, loss = 19.33068750\n",
      "Iteration 203, loss = 19.45597283\n",
      "Iteration 204, loss = 19.38860051\n",
      "Iteration 205, loss = 19.40288221\n",
      "Iteration 206, loss = 19.36001804\n",
      "Iteration 207, loss = 19.39641581\n",
      "Iteration 208, loss = 19.37535214\n",
      "Iteration 209, loss = 19.36719844\n",
      "Iteration 210, loss = 19.41046321\n",
      "Iteration 211, loss = 19.38019587\n",
      "Iteration 212, loss = 19.32388370\n",
      "Iteration 213, loss = 19.38406147\n",
      "Iteration 214, loss = 19.37674966\n",
      "Iteration 215, loss = 19.34957129\n",
      "Iteration 216, loss = 19.40756434\n",
      "Iteration 217, loss = 19.38438439\n",
      "Iteration 218, loss = 19.37757526\n",
      "Iteration 219, loss = 19.35872326\n",
      "Iteration 220, loss = 19.43741321\n",
      "Iteration 221, loss = 19.35473459\n",
      "Iteration 222, loss = 19.29400372\n",
      "Iteration 223, loss = 19.40324737\n",
      "Iteration 224, loss = 19.33348381\n",
      "Iteration 225, loss = 19.43834297\n",
      "Iteration 226, loss = 19.43710056\n",
      "Iteration 227, loss = 19.31979078\n",
      "Iteration 228, loss = 13.39157644\n",
      "Iteration 229, loss = 4.20120355\n",
      "Iteration 230, loss = 2.50110440\n",
      "Iteration 231, loss = 2.27185698\n",
      "Iteration 232, loss = 2.25925738\n",
      "Iteration 233, loss = 2.25438554\n",
      "Iteration 234, loss = 2.25412892\n",
      "Iteration 235, loss = 2.24746831\n",
      "Iteration 236, loss = 2.25426855\n",
      "Iteration 237, loss = 2.27069515\n",
      "Iteration 238, loss = 2.23320538\n",
      "Iteration 239, loss = 2.24500119\n",
      "Iteration 240, loss = 2.21763667\n",
      "Iteration 241, loss = 2.22868079\n",
      "Iteration 242, loss = 2.22060256\n",
      "Iteration 243, loss = 2.20560574\n",
      "Iteration 244, loss = 2.21859596\n",
      "Iteration 245, loss = 2.19961150\n",
      "Iteration 246, loss = 2.21377042\n",
      "Iteration 247, loss = 2.20674461\n",
      "Iteration 248, loss = 2.21050385\n",
      "Iteration 249, loss = 2.20902574\n",
      "Iteration 250, loss = 2.19051058\n",
      "Iteration 251, loss = 2.21513916\n",
      "Iteration 252, loss = 2.18642324\n",
      "Iteration 253, loss = 2.18771765\n",
      "Iteration 254, loss = 2.21165166\n",
      "Iteration 255, loss = 2.17384865\n",
      "Iteration 256, loss = 2.19439995\n",
      "Iteration 257, loss = 2.17293385\n",
      "Iteration 258, loss = 2.18331312\n",
      "Iteration 259, loss = 2.18492557\n",
      "Iteration 260, loss = 2.19386874\n",
      "Iteration 261, loss = 2.18540234\n",
      "Iteration 262, loss = 2.19187998\n",
      "Iteration 263, loss = 2.17178410\n",
      "Iteration 264, loss = 2.18078593\n",
      "Iteration 265, loss = 2.17164709\n",
      "Iteration 266, loss = 2.18746525\n",
      "Iteration 267, loss = 2.20494969\n",
      "Iteration 268, loss = 2.16549009\n",
      "Iteration 269, loss = 2.18224226\n",
      "Iteration 270, loss = 2.15353713\n",
      "Iteration 271, loss = 2.19565316\n",
      "Iteration 272, loss = 2.18678441\n",
      "Iteration 273, loss = 2.18613807\n",
      "Iteration 274, loss = 2.16927990\n",
      "Iteration 275, loss = 2.17867314\n",
      "Iteration 276, loss = 2.15919612\n",
      "Iteration 277, loss = 2.18383338\n",
      "Iteration 278, loss = 2.16720794\n",
      "Iteration 279, loss = 2.18036782\n",
      "Iteration 280, loss = 2.17778528\n",
      "Iteration 281, loss = 2.16637779\n",
      "Iteration 282, loss = 2.17662795\n",
      "Iteration 283, loss = 2.17832422\n",
      "Iteration 284, loss = 2.15919099\n",
      "Iteration 285, loss = 2.16460466\n",
      "Iteration 286, loss = 2.16626487\n",
      "Iteration 287, loss = 2.16320166\n",
      "Iteration 288, loss = 2.18968548\n",
      "Iteration 289, loss = 2.18341328\n",
      "Iteration 290, loss = 2.15832644\n",
      "Iteration 291, loss = 2.14313603\n",
      "Iteration 292, loss = 2.16930998\n",
      "Iteration 293, loss = 2.15523592\n",
      "Iteration 294, loss = 2.16083436\n",
      "Iteration 295, loss = 2.17860928\n",
      "Iteration 296, loss = 2.17591158\n",
      "Iteration 297, loss = 2.15787783\n",
      "Iteration 298, loss = 2.16440536\n",
      "Iteration 299, loss = 2.17036336\n",
      "Iteration 300, loss = 2.17700608\n",
      "Iteration 301, loss = 2.16327604\n",
      "Iteration 302, loss = 2.16529251\n",
      "Iteration 303, loss = 2.18017983\n",
      "Iteration 304, loss = 2.18125608\n",
      "Iteration 305, loss = 2.15627360\n",
      "Iteration 306, loss = 2.17896825\n",
      "Iteration 307, loss = 2.15832859\n",
      "Iteration 308, loss = 2.17297059\n",
      "Iteration 309, loss = 2.16860964\n",
      "Iteration 310, loss = 2.16797260\n",
      "Iteration 311, loss = 2.14846353\n",
      "Iteration 312, loss = 2.17414752\n",
      "Iteration 313, loss = 2.16663599\n",
      "Iteration 314, loss = 2.18067657\n",
      "Iteration 315, loss = 2.16488828\n",
      "Iteration 316, loss = 2.15073380\n",
      "Iteration 317, loss = 2.15563398\n",
      "Iteration 318, loss = 2.15581364\n",
      "Iteration 319, loss = 2.15221078\n",
      "Iteration 320, loss = 2.18332683\n",
      "Iteration 321, loss = 2.15029269\n",
      "Iteration 322, loss = 2.18294902\n",
      "Iteration 323, loss = 2.15230611\n",
      "Iteration 324, loss = 2.15146315\n",
      "Iteration 325, loss = 2.10762411\n",
      "Iteration 326, loss = 2.08914682\n",
      "Iteration 327, loss = 2.09255584\n",
      "Iteration 328, loss = 2.07053049\n",
      "Iteration 329, loss = 2.07314208\n",
      "Iteration 330, loss = 2.06735511\n",
      "Iteration 331, loss = 2.08419050\n",
      "Iteration 332, loss = 2.05743729\n",
      "Iteration 333, loss = 2.04455927\n",
      "Iteration 334, loss = 2.06709789\n",
      "Iteration 335, loss = 2.06218112\n",
      "Iteration 336, loss = 2.06583863\n",
      "Iteration 337, loss = 2.05440714\n",
      "Iteration 338, loss = 2.06365607\n",
      "Iteration 339, loss = 2.06273263\n",
      "Iteration 340, loss = 2.07568532\n",
      "Iteration 341, loss = 2.04624775\n",
      "Iteration 342, loss = 2.03615324\n",
      "Iteration 343, loss = 2.05007149\n",
      "Iteration 344, loss = 2.05984162\n",
      "Iteration 345, loss = 2.03072556\n",
      "Iteration 346, loss = 2.04940039\n",
      "Iteration 347, loss = 2.06524605\n",
      "Iteration 348, loss = 2.06307360\n",
      "Iteration 349, loss = 2.03940659\n",
      "Iteration 350, loss = 2.03833479\n",
      "Iteration 351, loss = 2.04561819\n",
      "Iteration 352, loss = 2.05625557\n",
      "Iteration 353, loss = 2.07240672\n",
      "Iteration 354, loss = 2.03677634\n",
      "Iteration 355, loss = 2.04145138\n",
      "Iteration 356, loss = 2.04984563\n",
      "Iteration 357, loss = 2.03500290\n",
      "Iteration 358, loss = 2.04893818\n",
      "Iteration 359, loss = 2.06074572\n",
      "Iteration 360, loss = 2.04864085\n",
      "Iteration 361, loss = 2.03325494\n",
      "Iteration 362, loss = 2.05380222\n",
      "Iteration 363, loss = 2.02936642\n",
      "Iteration 364, loss = 2.01982443\n",
      "Iteration 365, loss = 2.04992685\n",
      "Iteration 366, loss = 2.03424081\n",
      "Iteration 367, loss = 2.07042985\n",
      "Iteration 368, loss = 2.03266747\n",
      "Iteration 369, loss = 2.05575478\n",
      "Iteration 370, loss = 2.08622721\n",
      "Iteration 371, loss = 2.04522706\n",
      "Iteration 372, loss = 2.03247000\n",
      "Iteration 373, loss = 2.05420827\n",
      "Iteration 374, loss = 2.03647353\n",
      "Iteration 375, loss = 2.04554299\n",
      "Iteration 376, loss = 2.04783322\n",
      "Iteration 377, loss = 2.03889927\n",
      "Iteration 378, loss = 2.04511867\n",
      "Iteration 379, loss = 2.04739918\n",
      "Iteration 380, loss = 2.06184691\n",
      "Iteration 381, loss = 2.03851006\n",
      "Iteration 382, loss = 2.03731626\n",
      "Iteration 383, loss = 2.04538765\n",
      "Iteration 384, loss = 2.05766291\n",
      "Iteration 385, loss = 2.04522967\n",
      "Iteration 386, loss = 2.03890802\n",
      "Iteration 387, loss = 2.03177849\n",
      "Iteration 388, loss = 2.04371179\n",
      "Iteration 389, loss = 2.04119165\n",
      "Iteration 390, loss = 2.06100567\n",
      "Iteration 391, loss = 2.03165791\n",
      "Iteration 392, loss = 2.05075876\n",
      "Iteration 393, loss = 2.05239322\n",
      "Iteration 394, loss = 2.03992150\n",
      "Iteration 395, loss = 2.03175182\n",
      "Iteration 396, loss = 2.04347991\n",
      "Iteration 397, loss = 2.04296502\n",
      "Iteration 398, loss = 2.02735450\n",
      "Iteration 399, loss = 2.03178776\n",
      "Iteration 400, loss = 2.02235459\n",
      "Iteration 401, loss = 2.04392915\n",
      "Iteration 402, loss = 2.04772657\n",
      "Iteration 403, loss = 2.04216960\n",
      "Iteration 404, loss = 2.03925949\n",
      "Iteration 405, loss = 2.05882732\n",
      "Iteration 406, loss = 2.02388167\n",
      "Iteration 407, loss = 2.04762224\n",
      "Iteration 408, loss = 2.02179774\n",
      "Iteration 409, loss = 2.03240593\n",
      "Iteration 410, loss = 2.05082795\n",
      "Iteration 411, loss = 2.04222636\n",
      "Iteration 412, loss = 2.03823624\n",
      "Iteration 413, loss = 2.05477314\n",
      "Iteration 414, loss = 2.03944980\n",
      "Iteration 415, loss = 2.03249061\n",
      "Iteration 416, loss = 2.03641355\n",
      "Iteration 417, loss = 2.01967142\n",
      "Iteration 418, loss = 2.04147294\n",
      "Iteration 419, loss = 2.04061874\n",
      "Iteration 420, loss = 2.02357942\n",
      "Iteration 421, loss = 2.04250071\n",
      "Iteration 422, loss = 2.04032873\n",
      "Iteration 423, loss = 2.05086249\n",
      "Iteration 424, loss = 2.02843853\n",
      "Iteration 425, loss = 2.03895477\n",
      "Iteration 426, loss = 2.02039517\n",
      "Iteration 427, loss = 2.03061499\n",
      "Iteration 428, loss = 2.04666582\n",
      "Iteration 429, loss = 2.03632657\n",
      "Iteration 430, loss = 2.03537658\n",
      "Iteration 431, loss = 2.05404192\n",
      "Iteration 432, loss = 2.04359310\n",
      "Iteration 433, loss = 2.03235706\n",
      "Iteration 434, loss = 2.04256868\n",
      "Iteration 435, loss = 2.03651896\n",
      "Iteration 436, loss = 2.04521964\n",
      "Iteration 437, loss = 2.03110734\n",
      "Iteration 438, loss = 2.02637944\n",
      "Iteration 439, loss = 2.04398331\n",
      "Iteration 440, loss = 2.02845885\n",
      "Iteration 441, loss = 2.04666813\n",
      "Iteration 442, loss = 2.04735523\n",
      "Iteration 443, loss = 2.04858706\n",
      "Iteration 444, loss = 2.02561617\n",
      "Iteration 445, loss = 2.04264677\n",
      "Iteration 446, loss = 2.03796116\n",
      "Iteration 447, loss = 2.03631905\n",
      "Iteration 448, loss = 2.05204748\n",
      "Iteration 449, loss = 2.03563699\n",
      "Iteration 450, loss = 2.03738992\n",
      "Iteration 451, loss = 2.02866415\n",
      "Iteration 452, loss = 2.03708820\n",
      "Iteration 453, loss = 2.05233779\n",
      "Iteration 454, loss = 2.02769126\n",
      "Iteration 455, loss = 2.02780256\n",
      "Iteration 456, loss = 2.03232454\n",
      "Iteration 457, loss = 2.03927029\n",
      "Iteration 458, loss = 2.03067646\n",
      "Iteration 459, loss = 2.04934848\n",
      "Iteration 460, loss = 2.04526162\n",
      "Iteration 461, loss = 2.02984951\n",
      "Iteration 462, loss = 2.03401616\n",
      "Iteration 463, loss = 2.02398003\n",
      "Iteration 464, loss = 2.03840837\n",
      "Iteration 465, loss = 2.04041074\n",
      "Iteration 466, loss = 2.03157396\n",
      "Iteration 467, loss = 2.03681234\n",
      "Iteration 468, loss = 2.02818891\n",
      "Iteration 469, loss = 2.02946941\n",
      "Iteration 470, loss = 2.04635083\n",
      "Iteration 471, loss = 2.04119590\n",
      "Iteration 472, loss = 2.04083850\n",
      "Iteration 473, loss = 2.03780013\n",
      "Iteration 474, loss = 2.02963313\n",
      "Iteration 475, loss = 2.03096434\n",
      "Iteration 476, loss = 2.03865683\n",
      "Iteration 477, loss = 2.03288649\n",
      "Iteration 478, loss = 2.04006775\n",
      "Iteration 479, loss = 2.03631578\n",
      "Iteration 480, loss = 2.03118076\n",
      "Iteration 481, loss = 2.03881175\n",
      "Iteration 482, loss = 2.04887678\n",
      "Iteration 483, loss = 2.03280029\n",
      "Iteration 484, loss = 2.03907918\n",
      "Iteration 485, loss = 2.04284780\n",
      "Iteration 486, loss = 2.02713534\n",
      "Iteration 487, loss = 2.02262131\n",
      "Iteration 488, loss = 2.03058802\n",
      "Iteration 489, loss = 2.02886053\n",
      "Iteration 490, loss = 2.03869660\n",
      "Iteration 491, loss = 2.02823693\n",
      "Iteration 492, loss = 2.03015078\n",
      "Iteration 493, loss = 2.02681052\n",
      "Iteration 494, loss = 2.02169672\n",
      "Iteration 495, loss = 2.03774916\n",
      "Iteration 496, loss = 2.02655817\n",
      "Iteration 497, loss = 2.03830823\n",
      "Iteration 498, loss = 2.04307341\n",
      "Iteration 499, loss = 2.02928334\n",
      "Iteration 500, loss = 2.05524505\n",
      "Iteration 501, loss = 2.01071060\n",
      "Iteration 502, loss = 2.02473858\n",
      "Iteration 503, loss = 2.00522346\n",
      "Iteration 504, loss = 2.02395281\n",
      "Iteration 505, loss = 2.00860910\n",
      "Iteration 506, loss = 2.02260688\n",
      "Iteration 507, loss = 2.01272154\n",
      "Iteration 508, loss = 2.02666243\n",
      "Iteration 509, loss = 2.02735630\n",
      "Iteration 510, loss = 2.01499158\n",
      "Iteration 511, loss = 1.99287290\n",
      "Iteration 512, loss = 1.99643829\n",
      "Iteration 513, loss = 2.01606354\n",
      "Iteration 514, loss = 2.01390862\n",
      "Iteration 515, loss = 1.99093747\n",
      "Iteration 516, loss = 1.96274845\n",
      "Iteration 517, loss = 1.95638530\n",
      "Iteration 518, loss = 1.95203065\n",
      "Iteration 519, loss = 1.95451134\n",
      "Iteration 520, loss = 1.94211156\n",
      "Iteration 521, loss = 1.97957733\n",
      "Iteration 522, loss = 1.94799120\n",
      "Iteration 523, loss = 1.93628327\n",
      "Iteration 524, loss = 1.94391088\n",
      "Iteration 525, loss = 1.94557101\n",
      "Iteration 526, loss = 1.93824653\n",
      "Iteration 527, loss = 1.92112707\n",
      "Iteration 528, loss = 1.90833035\n",
      "Iteration 529, loss = 1.92795267\n",
      "Iteration 530, loss = 1.93312409\n",
      "Iteration 531, loss = 1.91835666\n",
      "Iteration 532, loss = 1.92413871\n",
      "Iteration 533, loss = 1.91159513\n",
      "Iteration 534, loss = 1.92574341\n",
      "Iteration 535, loss = 1.91518978\n",
      "Iteration 536, loss = 1.91605777\n",
      "Iteration 537, loss = 1.89643422\n",
      "Iteration 538, loss = 1.90653863\n",
      "Iteration 539, loss = 1.89548207\n",
      "Iteration 540, loss = 1.89642084\n",
      "Iteration 541, loss = 1.90105625\n",
      "Iteration 542, loss = 1.90103932\n",
      "Iteration 543, loss = 1.90243136\n",
      "Iteration 544, loss = 1.92172445\n",
      "Iteration 545, loss = 1.89242930\n",
      "Iteration 546, loss = 1.90881432\n",
      "Iteration 547, loss = 1.89870652\n",
      "Iteration 548, loss = 1.91297685\n",
      "Iteration 549, loss = 1.89544971\n",
      "Iteration 550, loss = 1.88740023\n",
      "Iteration 551, loss = 1.89758664\n",
      "Iteration 552, loss = 1.91026663\n",
      "Iteration 553, loss = 1.90653124\n",
      "Iteration 554, loss = 1.89565770\n",
      "Iteration 555, loss = 1.89078515\n",
      "Iteration 556, loss = 1.89046596\n",
      "Iteration 557, loss = 1.89119776\n",
      "Iteration 558, loss = 1.88483466\n",
      "Iteration 559, loss = 1.90587515\n",
      "Iteration 560, loss = 1.89694316\n",
      "Iteration 561, loss = 1.88198390\n",
      "Iteration 562, loss = 1.88586549\n",
      "Iteration 563, loss = 1.89315213\n",
      "Iteration 564, loss = 1.91402860\n",
      "Iteration 565, loss = 1.89548586\n",
      "Iteration 566, loss = 1.88634023\n",
      "Iteration 567, loss = 1.90918070\n",
      "Iteration 568, loss = 1.88023234\n",
      "Iteration 569, loss = 1.88811515\n",
      "Iteration 570, loss = 1.87539625\n",
      "Iteration 571, loss = 1.87319244\n",
      "Iteration 572, loss = 1.89164200\n",
      "Iteration 573, loss = 1.89327470\n",
      "Iteration 574, loss = 1.88532247\n",
      "Iteration 575, loss = 1.88768193\n",
      "Iteration 576, loss = 1.89382395\n",
      "Iteration 577, loss = 1.88893755\n",
      "Iteration 578, loss = 1.89189081\n",
      "Iteration 579, loss = 1.88450227\n",
      "Iteration 580, loss = 1.89099895\n",
      "Iteration 581, loss = 1.87887403\n",
      "Iteration 582, loss = 1.87959528\n",
      "Iteration 583, loss = 1.88755196\n",
      "Iteration 584, loss = 1.89252551\n",
      "Iteration 585, loss = 1.87876585\n",
      "Iteration 586, loss = 1.87862082\n",
      "Iteration 587, loss = 1.87991251\n",
      "Iteration 588, loss = 1.88168046\n",
      "Iteration 589, loss = 1.88277011\n",
      "Iteration 590, loss = 1.88456360\n",
      "Iteration 591, loss = 1.88390991\n",
      "Iteration 592, loss = 1.88762124\n",
      "Iteration 593, loss = 1.88693931\n",
      "Iteration 594, loss = 1.87560429\n",
      "Iteration 595, loss = 1.88809649\n",
      "Iteration 596, loss = 1.88367780\n",
      "Iteration 597, loss = 1.89060278\n",
      "Iteration 598, loss = 1.88490943\n",
      "Iteration 599, loss = 1.88135263\n",
      "Iteration 600, loss = 1.89395000\n",
      "Iteration 601, loss = 1.89282422\n",
      "Iteration 602, loss = 1.87735571\n",
      "Iteration 603, loss = 1.86699196\n",
      "Iteration 604, loss = 1.86707740\n",
      "Iteration 605, loss = 1.87326541\n",
      "Iteration 606, loss = 1.87368131\n",
      "Iteration 607, loss = 1.87469793\n",
      "Iteration 608, loss = 1.86848278\n",
      "Iteration 609, loss = 1.87303386\n",
      "Iteration 610, loss = 1.87086974\n",
      "Iteration 611, loss = 1.88020951\n",
      "Iteration 612, loss = 1.86925021\n",
      "Iteration 613, loss = 1.89471270\n",
      "Iteration 614, loss = 1.86470875\n",
      "Iteration 615, loss = 1.89584427\n",
      "Iteration 616, loss = 1.88336893\n",
      "Iteration 617, loss = 1.87123150\n",
      "Iteration 618, loss = 1.87790414\n",
      "Iteration 619, loss = 1.86084449\n",
      "Iteration 620, loss = 1.87614942\n",
      "Iteration 621, loss = 1.87390180\n",
      "Iteration 622, loss = 1.86363763\n",
      "Iteration 623, loss = 1.84605419\n",
      "Iteration 624, loss = 1.85874371\n",
      "Iteration 625, loss = 1.84109681\n",
      "Iteration 626, loss = 1.84686507\n",
      "Iteration 627, loss = 1.84592578\n",
      "Iteration 628, loss = 1.85314985\n",
      "Iteration 629, loss = 1.84950873\n",
      "Iteration 630, loss = 1.86406745\n",
      "Iteration 631, loss = 1.83658384\n",
      "Iteration 632, loss = 1.81631403\n",
      "Iteration 633, loss = 1.83504458\n",
      "Iteration 634, loss = 1.82842272\n",
      "Iteration 635, loss = 1.84390142\n",
      "Iteration 636, loss = 1.82765888\n",
      "Iteration 637, loss = 1.82890704\n",
      "Iteration 638, loss = 1.80187925\n",
      "Iteration 639, loss = 1.82079525\n",
      "Iteration 640, loss = 1.81142964\n",
      "Iteration 641, loss = 1.82180912\n",
      "Iteration 642, loss = 1.80644815\n",
      "Iteration 643, loss = 1.81738988\n",
      "Iteration 644, loss = 1.81769308\n",
      "Iteration 645, loss = 1.80054688\n",
      "Iteration 646, loss = 1.80466617\n",
      "Iteration 647, loss = 1.78344767\n",
      "Iteration 648, loss = 1.78580016\n",
      "Iteration 649, loss = 1.80293974\n",
      "Iteration 650, loss = 1.79520517\n",
      "Iteration 651, loss = 1.77585841\n",
      "Iteration 652, loss = 1.78455413\n",
      "Iteration 653, loss = 1.78226494\n",
      "Iteration 654, loss = 1.78949429\n",
      "Iteration 655, loss = 1.77990017\n",
      "Iteration 656, loss = 1.78405135\n",
      "Iteration 657, loss = 1.79228375\n",
      "Iteration 658, loss = 1.79050782\n",
      "Iteration 659, loss = 1.76579373\n",
      "Iteration 660, loss = 1.77173348\n",
      "Iteration 661, loss = 1.78041554\n",
      "Iteration 662, loss = 1.76356027\n",
      "Iteration 663, loss = 1.76918746\n",
      "Iteration 664, loss = 1.77731599\n",
      "Iteration 665, loss = 1.77914367\n",
      "Iteration 666, loss = 1.76506474\n",
      "Iteration 667, loss = 1.75907847\n",
      "Iteration 668, loss = 1.76331784\n",
      "Iteration 669, loss = 1.78288538\n",
      "Iteration 670, loss = 1.76248584\n",
      "Iteration 671, loss = 1.74603516\n",
      "Iteration 672, loss = 1.76909948\n",
      "Iteration 673, loss = 1.75184216\n",
      "Iteration 674, loss = 1.74765582\n",
      "Iteration 675, loss = 1.74717980\n",
      "Iteration 676, loss = 1.74120045\n",
      "Iteration 677, loss = 1.75094068\n",
      "Iteration 678, loss = 1.73826102\n",
      "Iteration 679, loss = 1.74359288\n",
      "Iteration 680, loss = 1.73762694\n",
      "Iteration 681, loss = 1.73983665\n",
      "Iteration 682, loss = 1.73507570\n",
      "Iteration 683, loss = 1.71108353\n",
      "Iteration 684, loss = 1.69877129\n",
      "Iteration 685, loss = 1.68810919\n",
      "Iteration 686, loss = 1.66553747\n",
      "Iteration 687, loss = 1.63942109\n",
      "Iteration 688, loss = 1.61249023\n",
      "Iteration 689, loss = 1.57504333\n",
      "Iteration 690, loss = 1.54697090\n",
      "Iteration 691, loss = 1.52312697\n",
      "Iteration 692, loss = 1.52185033\n",
      "Iteration 693, loss = 1.50794258\n",
      "Iteration 694, loss = 1.49538948\n",
      "Iteration 695, loss = 1.49035384\n",
      "Iteration 696, loss = 1.48525777\n",
      "Iteration 697, loss = 1.47025806\n",
      "Iteration 698, loss = 1.44453368\n",
      "Iteration 699, loss = 1.43897901\n",
      "Iteration 700, loss = 1.44785831\n",
      "Iteration 701, loss = 1.43396753\n",
      "Iteration 702, loss = 1.42200722\n",
      "Iteration 703, loss = 1.41631558\n",
      "Iteration 704, loss = 1.41244466\n",
      "Iteration 705, loss = 1.41418126\n",
      "Iteration 706, loss = 1.39207128\n",
      "Iteration 707, loss = 1.40854278\n",
      "Iteration 708, loss = 1.38567724\n",
      "Iteration 709, loss = 1.40584257\n",
      "Iteration 710, loss = 1.38844342\n",
      "Iteration 711, loss = 1.39182079\n",
      "Iteration 712, loss = 1.37320089\n",
      "Iteration 713, loss = 1.39184234\n",
      "Iteration 714, loss = 1.37981296\n",
      "Iteration 715, loss = 1.37552642\n",
      "Iteration 716, loss = 1.37317218\n",
      "Iteration 717, loss = 1.38194686\n",
      "Iteration 718, loss = 1.37492999\n",
      "Iteration 719, loss = 1.38978206\n",
      "Iteration 720, loss = 1.37122520\n",
      "Iteration 721, loss = 1.36558493\n",
      "Iteration 722, loss = 1.36386087\n",
      "Iteration 723, loss = 1.36314647\n",
      "Iteration 724, loss = 1.36939347\n",
      "Iteration 725, loss = 1.36868033\n",
      "Iteration 726, loss = 1.36107880\n",
      "Iteration 727, loss = 1.36228371\n",
      "Iteration 728, loss = 1.36082834\n",
      "Iteration 729, loss = 1.35650300\n",
      "Iteration 730, loss = 1.36023794\n",
      "Iteration 731, loss = 1.35021750\n",
      "Iteration 732, loss = 1.35528478\n",
      "Iteration 733, loss = 1.35898246\n",
      "Iteration 734, loss = 1.37029076\n",
      "Iteration 735, loss = 1.34637226\n",
      "Iteration 736, loss = 1.36868380\n",
      "Iteration 737, loss = 1.34639778\n",
      "Iteration 738, loss = 1.36646940\n",
      "Iteration 739, loss = 1.35035407\n",
      "Iteration 740, loss = 1.36234888\n",
      "Iteration 741, loss = 1.36553658\n",
      "Iteration 742, loss = 1.36122328\n",
      "Iteration 743, loss = 1.34619203\n",
      "Iteration 744, loss = 1.35675611\n",
      "Iteration 745, loss = 1.36848486\n",
      "Iteration 746, loss = 1.35865297\n",
      "Iteration 747, loss = 1.37043677\n",
      "Iteration 748, loss = 1.35947615\n",
      "Iteration 749, loss = 1.35672049\n",
      "Iteration 750, loss = 1.35254866\n",
      "Iteration 751, loss = 1.36084914\n",
      "Iteration 752, loss = 1.35321022\n",
      "Iteration 753, loss = 1.35382366\n",
      "Iteration 754, loss = 1.35341303\n",
      "Iteration 755, loss = 1.35406331\n",
      "Iteration 756, loss = 1.36388292\n",
      "Iteration 757, loss = 1.35837421\n",
      "Iteration 758, loss = 1.36187895\n",
      "Iteration 759, loss = 1.35145900\n",
      "Iteration 760, loss = 1.36723952\n",
      "Iteration 761, loss = 1.35248456\n",
      "Iteration 762, loss = 1.34984594\n",
      "Iteration 763, loss = 1.35795553\n",
      "Iteration 764, loss = 1.35209587\n",
      "Iteration 765, loss = 1.35077020\n",
      "Iteration 766, loss = 1.35099301\n",
      "Iteration 767, loss = 1.35446066\n",
      "Iteration 768, loss = 1.35399147\n",
      "Iteration 769, loss = 1.36097826\n",
      "Iteration 770, loss = 1.34728015\n",
      "Iteration 771, loss = 1.35779973\n",
      "Iteration 772, loss = 1.36271555\n",
      "Iteration 773, loss = 1.35759674\n",
      "Iteration 774, loss = 1.33280320\n",
      "Iteration 775, loss = 1.33440949\n",
      "Iteration 776, loss = 1.30894927\n",
      "Iteration 777, loss = 1.26944850\n",
      "Iteration 778, loss = 1.25151601\n",
      "Iteration 779, loss = 1.25236936\n",
      "Iteration 780, loss = 1.23043129\n",
      "Iteration 781, loss = 1.22954845\n",
      "Iteration 782, loss = 1.18604731\n",
      "Iteration 783, loss = 1.17962785\n",
      "Iteration 784, loss = 1.16275880\n",
      "Iteration 785, loss = 1.15100151\n",
      "Iteration 786, loss = 1.12656038\n",
      "Iteration 787, loss = 1.10428485\n",
      "Iteration 788, loss = 1.09158586\n",
      "Iteration 789, loss = 1.06881184\n",
      "Iteration 790, loss = 1.04765784\n",
      "Iteration 791, loss = 1.04965123\n",
      "Iteration 792, loss = 1.01646790\n",
      "Iteration 793, loss = 1.01348014\n",
      "Iteration 794, loss = 0.98718109\n",
      "Iteration 795, loss = 0.98462854\n",
      "Iteration 796, loss = 0.98240317\n",
      "Iteration 797, loss = 0.97668914\n",
      "Iteration 798, loss = 0.96392112\n",
      "Iteration 799, loss = 0.95364345\n",
      "Iteration 800, loss = 0.94365385\n",
      "Iteration 801, loss = 0.94794004\n",
      "Iteration 802, loss = 0.93180988\n",
      "Iteration 803, loss = 0.93593958\n",
      "Iteration 804, loss = 0.92052668\n",
      "Iteration 805, loss = 0.95361413\n",
      "Iteration 806, loss = 0.92001473\n",
      "Iteration 807, loss = 0.90725343\n",
      "Iteration 808, loss = 0.92474649\n",
      "Iteration 809, loss = 0.92090234\n",
      "Iteration 810, loss = 0.91360789\n",
      "Iteration 811, loss = 0.90208218\n",
      "Iteration 812, loss = 0.90352799\n",
      "Iteration 813, loss = 0.88929210\n",
      "Iteration 814, loss = 0.88370087\n",
      "Iteration 815, loss = 0.88598361\n",
      "Iteration 816, loss = 0.89825337\n",
      "Iteration 817, loss = 0.88198052\n",
      "Iteration 818, loss = 0.87987146\n",
      "Iteration 819, loss = 0.87479209\n",
      "Iteration 820, loss = 0.86592998\n",
      "Iteration 821, loss = 0.86997720\n",
      "Iteration 822, loss = 0.85563454\n",
      "Iteration 823, loss = 0.85936306\n",
      "Iteration 824, loss = 0.86823352\n",
      "Iteration 825, loss = 0.86755132\n",
      "Iteration 826, loss = 0.84472560\n",
      "Iteration 827, loss = 0.85614558\n",
      "Iteration 828, loss = 0.83278021\n",
      "Iteration 829, loss = 0.84411691\n",
      "Iteration 830, loss = 0.83672116\n",
      "Iteration 831, loss = 0.84168169\n",
      "Iteration 832, loss = 0.81891134\n",
      "Iteration 833, loss = 0.83900848\n",
      "Iteration 834, loss = 0.80720220\n",
      "Iteration 835, loss = 0.81088939\n",
      "Iteration 836, loss = 0.81615037\n",
      "Iteration 837, loss = 0.81753826\n",
      "Iteration 838, loss = 0.80015098\n",
      "Iteration 839, loss = 0.80234604\n",
      "Iteration 840, loss = 0.79310538\n",
      "Iteration 841, loss = 0.78823732\n",
      "Iteration 842, loss = 0.77714045\n",
      "Iteration 843, loss = 0.77428947\n",
      "Iteration 844, loss = 0.76234199\n",
      "Iteration 845, loss = 0.75209199\n",
      "Iteration 846, loss = 0.73897664\n",
      "Iteration 847, loss = 0.74917495\n",
      "Iteration 848, loss = 0.73679898\n",
      "Iteration 849, loss = 0.70193628\n",
      "Iteration 850, loss = 0.70621491\n",
      "Iteration 851, loss = 0.69379893\n",
      "Iteration 852, loss = 0.67250330\n",
      "Iteration 853, loss = 0.66746395\n",
      "Iteration 854, loss = 0.64071509\n",
      "Iteration 855, loss = 0.65277360\n",
      "Iteration 856, loss = 0.62913326\n",
      "Iteration 857, loss = 0.63024495\n",
      "Iteration 858, loss = 0.62824523\n",
      "Iteration 859, loss = 0.61299876\n",
      "Iteration 860, loss = 0.61187675\n",
      "Iteration 861, loss = 0.60980494\n",
      "Iteration 862, loss = 0.60901742\n",
      "Iteration 863, loss = 0.60894086\n",
      "Iteration 864, loss = 0.60100984\n",
      "Iteration 865, loss = 0.59224509\n",
      "Iteration 866, loss = 0.58749417\n",
      "Iteration 867, loss = 0.60346579\n",
      "Iteration 868, loss = 0.59178647\n",
      "Iteration 869, loss = 0.59725990\n",
      "Iteration 870, loss = 0.60770057\n",
      "Iteration 871, loss = 0.60123582\n",
      "Iteration 872, loss = 0.59784464\n",
      "Iteration 873, loss = 0.59846483\n",
      "Iteration 874, loss = 0.59524260\n",
      "Iteration 875, loss = 0.59797652\n",
      "Iteration 876, loss = 0.59796299\n",
      "Iteration 877, loss = 0.59698817\n",
      "Iteration 878, loss = 0.60178716\n",
      "Iteration 879, loss = 0.59070016\n",
      "Iteration 880, loss = 0.59086597\n",
      "Iteration 881, loss = 0.60162633\n",
      "Iteration 882, loss = 0.59284729\n",
      "Iteration 883, loss = 0.58832845\n",
      "Iteration 884, loss = 0.59247076\n",
      "Iteration 885, loss = 0.60254885\n",
      "Iteration 886, loss = 0.59619927\n",
      "Iteration 887, loss = 0.59490132\n",
      "Iteration 888, loss = 0.59273775\n",
      "Iteration 889, loss = 0.61331617\n",
      "Iteration 890, loss = 0.60688104\n",
      "Iteration 891, loss = 0.58987449\n",
      "Iteration 892, loss = 0.58255004\n",
      "Iteration 893, loss = 0.59953418\n",
      "Iteration 894, loss = 0.60321985\n",
      "Iteration 895, loss = 0.59898215\n",
      "Iteration 896, loss = 0.59508452\n",
      "Iteration 897, loss = 0.59409613\n",
      "Iteration 898, loss = 0.59705015\n",
      "Iteration 899, loss = 0.59701183\n",
      "Iteration 900, loss = 0.60073614\n",
      "Iteration 901, loss = 0.61020630\n",
      "Iteration 902, loss = 0.60054130\n",
      "Iteration 903, loss = 0.59847442\n",
      "Iteration 904, loss = 0.59444136\n",
      "Iteration 905, loss = 0.59023822\n",
      "Iteration 906, loss = 0.60397278\n",
      "Iteration 907, loss = 0.58949872\n",
      "Iteration 908, loss = 0.59408870\n",
      "Iteration 909, loss = 0.60082282\n",
      "Iteration 910, loss = 0.59165543\n",
      "Iteration 911, loss = 0.58710638\n",
      "Iteration 912, loss = 0.58797055\n",
      "Iteration 913, loss = 0.59273496\n",
      "Iteration 914, loss = 0.59410679\n",
      "Iteration 915, loss = 0.59839963\n",
      "Iteration 916, loss = 0.59058418\n",
      "Iteration 917, loss = 0.58126045\n",
      "Iteration 918, loss = 0.59420848\n",
      "Iteration 919, loss = 0.59206396\n",
      "Iteration 920, loss = 0.59001095\n",
      "Iteration 921, loss = 0.59657161\n",
      "Iteration 922, loss = 0.59543845\n",
      "Iteration 923, loss = 0.59635389\n",
      "Iteration 924, loss = 0.58780010\n",
      "Iteration 925, loss = 0.58709025\n",
      "Iteration 926, loss = 0.59842353\n",
      "Iteration 927, loss = 0.59025006\n",
      "Iteration 928, loss = 0.58251807\n",
      "Iteration 929, loss = 0.59696827\n",
      "Iteration 930, loss = 0.60167638\n",
      "Iteration 931, loss = 0.59558604\n",
      "Iteration 932, loss = 0.59383795\n",
      "Iteration 933, loss = 0.59140343\n",
      "Iteration 934, loss = 0.59237351\n",
      "Iteration 935, loss = 0.59855346\n",
      "Iteration 936, loss = 0.59154392\n",
      "Iteration 937, loss = 0.58844734\n",
      "Iteration 938, loss = 0.59525900\n",
      "Iteration 939, loss = 0.60098569\n",
      "Iteration 940, loss = 0.59753606\n",
      "Iteration 941, loss = 0.59160653\n",
      "Iteration 942, loss = 0.58176158\n",
      "Iteration 943, loss = 0.58547114\n",
      "Iteration 944, loss = 0.58484465\n",
      "Iteration 945, loss = 0.59787773\n",
      "Iteration 946, loss = 0.58279837\n",
      "Iteration 947, loss = 0.59511907\n",
      "Iteration 948, loss = 0.58475171\n",
      "Iteration 949, loss = 0.59193435\n",
      "Iteration 950, loss = 0.58992205\n",
      "Iteration 951, loss = 0.58390808\n",
      "Iteration 952, loss = 0.58241920\n",
      "Iteration 953, loss = 0.59111722\n",
      "Iteration 954, loss = 0.59886457\n",
      "Iteration 955, loss = 0.58745557\n",
      "Iteration 956, loss = 0.59064542\n",
      "Iteration 957, loss = 0.59242366\n",
      "Iteration 958, loss = 0.58125390\n",
      "Iteration 959, loss = 0.58232679\n",
      "Iteration 960, loss = 0.58322100\n",
      "Iteration 961, loss = 0.59421672\n",
      "Iteration 962, loss = 0.58649680\n",
      "Iteration 963, loss = 0.60129205\n",
      "Iteration 964, loss = 0.58532247\n",
      "Iteration 965, loss = 0.58859266\n",
      "Iteration 966, loss = 0.59584498\n",
      "Iteration 967, loss = 0.58802082\n",
      "Iteration 968, loss = 0.58749328\n",
      "Iteration 969, loss = 0.60443215\n",
      "Iteration 970, loss = 0.58583060\n",
      "Iteration 971, loss = 0.59234290\n",
      "Iteration 972, loss = 0.58775218\n",
      "Iteration 973, loss = 0.59050560\n",
      "Iteration 974, loss = 0.59191617\n",
      "Iteration 975, loss = 0.60316153\n",
      "Iteration 976, loss = 0.59340220\n",
      "Iteration 977, loss = 0.58672353\n",
      "Iteration 978, loss = 0.59652459\n",
      "Iteration 979, loss = 0.58621172\n",
      "Iteration 980, loss = 0.58868438\n",
      "Iteration 981, loss = 0.58761946\n",
      "Iteration 982, loss = 0.58911436\n",
      "Iteration 983, loss = 0.58158529\n",
      "Iteration 984, loss = 0.58779460\n",
      "Iteration 985, loss = 0.58813758\n",
      "Iteration 986, loss = 0.58639594\n",
      "Iteration 987, loss = 0.58881619\n",
      "Iteration 988, loss = 0.58862028\n",
      "Iteration 989, loss = 0.58277791\n",
      "Iteration 990, loss = 0.58633505\n",
      "Iteration 991, loss = 0.57736899\n",
      "Iteration 992, loss = 0.58367815\n",
      "Iteration 993, loss = 0.57847941\n",
      "Iteration 994, loss = 0.59240847\n",
      "Iteration 995, loss = 0.58350867\n",
      "Iteration 996, loss = 0.58101063\n",
      "Iteration 997, loss = 0.59776982\n",
      "Iteration 998, loss = 0.58893854\n",
      "Iteration 999, loss = 0.58221650\n",
      "Iteration 1000, loss = 0.59468985\n",
      "0.9947044217202251\n",
      "Happinness Model trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/ETH/wunderkind/WunderkindBackend/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89045c7d9cb453b8bc972519b053be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c514cc2402ee40e691ee18de804093fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General RL model trained\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual RL model trained\n",
      "Observation:  [2.7, 1.0, 5.0, 0, 74.25]\n",
      "Observation:  [2.2, 1.0, 5.0, 0, 74.25]\n",
      "Observation:  [2.7, 1.0, 5.0, 0, 74.25]\n",
      "Observation:  [2.7, 1.5, 5.0, 0, 74.25]\n",
      "Observation:  [2.2, 1.5, 5.0, 0, 74.25]\n",
      "Observation:  [1.7000000000000002, 1.5, 5.0, 0, 72.75]\n",
      "Observation:  [1.7000000000000002, 1.5, 5.0, 4, 97.75]\n",
      "Episode finished after 7 timesteps\n",
      "Observation:  [2.2, 1.0, 5.0, 2, 77.75]\n",
      "Observation:  [2.2, 1.0, 4.5, 2, 76.25]\n",
      "Observation:  [2.2, 0.5, 4.5, 2, 65.75]\n",
      "Observation:  [2.2, 1.0, 4.5, 2, 76.25]\n",
      "Observation:  [2.2, 1.5, 4.5, 2, 87.5]\n",
      "Observation:  [2.2, 1.5, 5.0, 2, 87.5]\n",
      "Observation:  [2.2, 1.5, 5.5, 2, 87.5]\n",
      "Observation:  [1.7000000000000002, 1.5, 5.5, 2, 82.25]\n",
      "Observation:  [1.7000000000000002, 1.5, 5.0, 2, 83.75]\n",
      "Observation:  [2.2, 1.5, 5.0, 2, 87.5]\n",
      "Observation:  [2.2, 1.5, 4.5, 2, 87.5]\n",
      "Observation:  [2.2, 1.5, 5.0, 2, 87.5]\n",
      "Observation:  [2.2, 2.0, 5.0, 2, 87.5]\n",
      "Observation:  [2.2, 2.0, 4.5, 2, 87.5]\n",
      "Observation:  [2.2, 2.0, 4.5, 4, 100.0]\n",
      "Episode finished after 15 timesteps\n",
      "Mean finish time difference:  -0.08\n"
     ]
    }
   ],
   "source": [
    "happiness_score=[]\n",
    "eval_env = AppEnvironment()\n",
    "\n",
    "mean_finish_time_diff = 0\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    tmp_arr = []\n",
    "    obs = eval_env.reset()\n",
    "\n",
    "    ## assesment period\n",
    "    assesment_reading_time = 0\n",
    "    assesment_activity_time = 0\n",
    "\n",
    "    random_reading_time = round(random.random(),1)\n",
    "    random_activity_time = round(random.random(),1)\n",
    "\n",
    "    eval_env.observation = np.array([random_reading_time, random_activity_time, 5.0, 0, float(generate_happiness_score(0, 0, 5))])\n",
    "\n",
    "    for j in range(10):\n",
    "\n",
    "        action = random.randint(0,4)\n",
    "\n",
    "        obs, rewards, dones, _, _ = eval_env.step(action)\n",
    "        #print(\"Observation: \", obs)\n",
    "        tmp_arr.append(obs[4])\n",
    "        assesment_reading_time += obs[0]\n",
    "        assesment_activity_time += obs[1]\n",
    "        if dones:\n",
    "            print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "            break\n",
    "    \n",
    "\n",
    "    if dones:\n",
    "        continue\n",
    "    \n",
    "    assesment_reading_time /=10\n",
    "    assesment_activity_time /=10\n",
    "\n",
    "    assesment_reading_time = round(assesment_reading_time,1)\n",
    "    assesment_activity_time = round(assesment_activity_time,1)\n",
    "\n",
    "\n",
    "    print(\"Finished assesment period, reading time: \", assesment_reading_time, \"activity time: \", assesment_activity_time)\n",
    "    create_indiv_dataset(assesment_reading_time, assesment_activity_time)\n",
    "    print(\"Dataset created\")\n",
    "    train_hmodel()\n",
    "    print(\"Happinness Model trained\")\n",
    "    general_model = train_default_RL_model()\n",
    "    print(\"General RL model trained\")\n",
    "    eval_env.assessment = False\n",
    "    indiv_model = train_individual_RL_model()\n",
    "    print(\"Individual RL model trained\")\n",
    "    \n",
    "    tmp_arr_general = tmp_arr.copy()\n",
    "    tmp_obs = obs.copy()\n",
    "    \n",
    "    \n",
    "    ## with individual model ##\n",
    "\n",
    "    for j in range(100):\n",
    "        # BestSuggestions\n",
    "        action, _states = indiv_model.predict(obs)\n",
    "\n",
    "        # Child will sometimes consider suggestions of app or not\n",
    "        action = random.randint(2,4) if random.random() < 0.25 else action\n",
    "\n",
    "        obs, rewards, dones, info, _ = eval_env.step(action)\n",
    "        print(\"Observation: \", obs)\n",
    "        tmp_arr.append(obs[4])\n",
    "        if dones:\n",
    "            print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "            mean_finish_time_diff += j+1\n",
    "            break\n",
    "    happiness_score.append(tmp_arr)\n",
    "\n",
    "    ## with general model ##\n",
    "\n",
    "    eval_env.reset()\n",
    "    eval_env.observation = tmp_obs\n",
    "    eval_env.assessment = True\n",
    "    for j in range(100):\n",
    "        # Best Suggestions\n",
    "        action, _states = general_model.predict(obs)\n",
    "\n",
    "        # Child will sometimes consider suggestions of app or not\n",
    "        action = random.randint(2,4) if random.random() < 0.25 else action\n",
    "\n",
    "        obs, rewards, dones, info,_ = eval_env.step(action)\n",
    "        print(\"Observation: \", obs)\n",
    "        tmp_arr_general.append(obs[4])\n",
    "        if dones:\n",
    "            print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "            mean_finish_time_diff -= j+1\n",
    "            break\n",
    "    happiness_score.append(tmp_arr_general)\n",
    "\n",
    "print(\"Mean finish time difference: \", mean_finish_time_diff/100)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(happiness_score[i])\n",
    "plt.ylabel('Happiness Score')\n",
    "plt.xlabel('Days')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
